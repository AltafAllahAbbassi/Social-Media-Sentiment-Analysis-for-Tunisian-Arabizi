{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tunisian_sentiment_analysis_starter_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dflmqI7EElzi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d6d3c6a-8617-4045-8759-c6befaddafc7"
      },
      "source": [
        "!pip install texthero"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting texthero\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/5a/a9d33b799fe53011de79d140ad6d86c440a2da1ae8a7b24e851ee2f8bde8/texthero-1.0.9-py3-none-any.whl\n",
            "Requirement already satisfied: plotly>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (4.4.1)\n",
            "Requirement already satisfied: pandas>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.1.5)\n",
            "Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (3.2.2)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from texthero) (2.2.4)\n",
            "Requirement already satisfied: wordcloud>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.19.5)\n",
            "Collecting unidecode>=1.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: gensim>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.3 in /usr/local/lib/python3.7/dist-packages (from texthero) (4.41.1)\n",
            "Collecting nltk>=3.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from texthero) (0.22.2.post1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly>=4.2.0->texthero) (1.15.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly>=4.2.0->texthero) (1.3.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.2->texthero) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.2->texthero) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (2.4.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (0.8.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (2.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (53.0.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (3.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->texthero) (2.23.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud>=1.5.0->texthero) (7.0.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.6.0->texthero) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.6.0->texthero) (4.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.3->texthero) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.3->texthero) (1.0.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk>=3.3->texthero) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->texthero) (3.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->texthero) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->texthero) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->texthero) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->texthero) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->texthero) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->texthero) (3.4.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-cp37-none-any.whl size=1434672 sha256=182c9dd9d9921aabea15223e8ac5d004cc74ce66be1966813294c8bdc9c6b51b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
            "Successfully built nltk\n",
            "Installing collected packages: unidecode, nltk, texthero\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.5 texthero-1.0.9 unidecode-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJjzLUBRQfI2",
        "outputId": "c3e80148-7554-4658-b12e-19dfa9520c1e"
      },
      "source": [
        "# Handling datasets \r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "# Some encoding and preprocessing textual data\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "import texthero\r\n",
        "from texthero import preprocessing\r\n",
        "\r\n",
        "# Set of ML classifiers to be used\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn import svm\r\n",
        "from xgboost import XGBClassifier\r\n",
        "from lightgbm import LGBMClassifier"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7N_FxroQimt"
      },
      "source": [
        "def preprocess(data):\r\n",
        "    data = data.copy()\r\n",
        "    #removing diacritics\r\n",
        "    data['text'] = preprocessing.remove_diacritics(data['text'])\r\n",
        "    return data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4ave48oQqcz"
      },
      "source": [
        "# Read the training and testing datasets\r\n",
        "train_df = pd.read_csv('Train.csv')\r\n",
        "test_df = pd.read_csv('Test.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SayIMblBQlv2"
      },
      "source": [
        "# Preprocessing\r\n",
        "train = preprocess(train_df)\r\n",
        "test = preprocess(test_df)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGnE37QDQn5k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f8f755d-24de-4785-c91e-8f6860b9a15d"
      },
      "source": [
        "# Building a vextorizer object that allow us to transform the texts into a numerical representation (vector)\r\n",
        "# and fit it on the training pieces of texts\r\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 3)).fit(train['text'].values)\r\n",
        "len(vectorizer.vocabulary_)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1087890"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldCzxMEnRTPy"
      },
      "source": [
        "# Split the training dataset into training and validation\r\n",
        "X_train, X_val, y_train, y_val = train_test_split(train, train['label'], test_size=0.2, random_state=0,stratify=train['label'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWdAWFb-RwF2"
      },
      "source": [
        "# Transform training, validation and testing texts into vectors\r\n",
        "X_train = vectorizer.transform(X_train['text'].values)\r\n",
        "X_val = vectorizer.transform(X_val['text'].values)\r\n",
        "X_test = vectorizer.transform(test['text'].values)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMtDh9fYdLWr"
      },
      "source": [
        "You can try to tweak the model's parameters in order to get better performances I used the default setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a9TNFR_cIOM"
      },
      "source": [
        "# Logistic Regression\r\n",
        "\r\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "he2T1QVp3_mG",
        "outputId": "3aab9ba0-be31-4785-cecd-ca0a17c69fcb"
      },
      "source": [
        "clf1 = LogisticRegression(random_state=0)\r\n",
        "model1 = clf1.fit(X_train, y_train)\r\n",
        "pred1 = model1.predict(X_val)\r\n",
        "print(classification_report(y_val,pred1,digits=4))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.7357    0.8145    0.7731      5859\n",
            "           0     1.0000    0.0264    0.0514       493\n",
            "           1     0.8274    0.8115    0.8193      7648\n",
            "\n",
            "    accuracy                         0.7851     14000\n",
            "   macro avg     0.8544    0.5508    0.5479     14000\n",
            "weighted avg     0.7951    0.7851    0.7729     14000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning:\n",
            "\n",
            "lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMW5gdBidFTV"
      },
      "source": [
        "# SVM\r\n",
        "\r\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2tIKJyESefC",
        "outputId": "9f439410-4fca-485f-8b8e-ffa6033eff29"
      },
      "source": [
        "clf2 = svm.SVC(C=2)\r\n",
        "model2 = clf2.fit(X_train, y_train)\r\n",
        "pred2 = model2.predict(X_val)\r\n",
        "print(classification_report(y_val,pred2,digits=4))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.7371    0.8307    0.7811      5859\n",
            "           0     0.9310    0.0548    0.1034       493\n",
            "           1     0.8393    0.8086    0.8237      7648\n",
            "\n",
            "    accuracy                         0.7913     14000\n",
            "   macro avg     0.8358    0.5647    0.5694     14000\n",
            "weighted avg     0.7998    0.7913    0.7805     14000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSs2ZXQ2dkdq"
      },
      "source": [
        "# Random Forest\r\n",
        "\r\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en8N-VV06Yi-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b44d47c-cdf1-467a-b127-5aaa73b385d4"
      },
      "source": [
        "clf3 = RandomForestClassifier()\r\n",
        "model3 = clf3.fit(X_train, y_train)\r\n",
        "pred3 = model3.predict(X_val)\r\n",
        "print(classification_report(y_val,pred3,digits=4))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.7281    0.7247    0.7264      5859\n",
            "           0     0.7206    0.0994    0.1747       493\n",
            "           1     0.7707    0.8163    0.7929      7648\n",
            "\n",
            "    accuracy                         0.7527     14000\n",
            "   macro avg     0.7398    0.5468    0.5646     14000\n",
            "weighted avg     0.7511    0.7527    0.7433     14000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDZw2lpleDT7"
      },
      "source": [
        "# XGBoost\r\n",
        "\r\n",
        "https://xgboost.readthedocs.io/en/latest/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNjE57QN73rV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73d4dc52-daca-4159-acfc-c490f8a58d31"
      },
      "source": [
        "clf4 = XGBClassifier()\r\n",
        "model4 = clf4.fit(X_train, y_train)\r\n",
        "pred4 = model4.predict(X_val)\r\n",
        "print(classification_report(y_val,pred4,digits=4))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.7195    0.4554    0.5578      5859\n",
            "           0     0.7778    0.0284    0.0548       493\n",
            "           1     0.6529    0.8771    0.7486      7648\n",
            "\n",
            "    accuracy                         0.6707     14000\n",
            "   macro avg     0.7167    0.4536    0.4537     14000\n",
            "weighted avg     0.6852    0.6707    0.6443     14000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFQ074NNefSL"
      },
      "source": [
        "# LightGBM\r\n",
        "\r\n",
        "https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiOMnqj4flBH",
        "outputId": "ae80654c-ac5a-4cf3-fdbb-841ee3a9513e"
      },
      "source": [
        "clf5 = LGBMClassifier()\r\n",
        "model5 = clf5.fit(X_train, y_train)\r\n",
        "pred5 = model5.predict(X_val)\r\n",
        "print(classification_report(y_val,pred5,digits=4))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.6605    0.8396    0.7394      5859\n",
            "           0     0.6508    0.0832    0.1475       493\n",
            "           1     0.8365    0.7099    0.7680      7648\n",
            "\n",
            "    accuracy                         0.7421     14000\n",
            "   macro avg     0.7159    0.5442    0.5516     14000\n",
            "weighted avg     0.7563    0.7421    0.7342     14000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-Mbv_VmeuBi"
      },
      "source": [
        "# Make Prediction and create a submission file to submit it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayJlXzZaSrg0"
      },
      "source": [
        "# Predict on the test dataset based on one of your models SVM, RandomForest, XGBoost,...\r\n",
        "pred = model2.predict(X_test) # I used SVM classifier\r\n",
        "\r\n",
        "# Create the submission files with IDs and the prediction of your model\r\n",
        "test_df['label'] = pred\r\n",
        "submit = test_df[[\"ID\",\"label\"]]\r\n",
        "submit.to_csv(\"starter_submission.csv\", index=False)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1CrmyDUS0A8"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}